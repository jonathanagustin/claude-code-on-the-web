# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Overview

This is a research project investigating the feasibility of running Kubernetes (k3s) worker nodes in highly restricted sandbox environments (gVisor/runsc with 9p virtual filesystems), specifically targeting Claude Code web sessions.

**ğŸ‰ BREAKTHROUGH #1** (2025-11-22): Discovered that k3s requires CNI plugins even with `--disable-agent`. Created minimal fake CNI plugin that enables **native k3s control-plane** (no Docker required)!

**ğŸš€ BREAKTHROUGH #2** (2025-11-22): Experiment 13 resolved ALL 6 fundamental blockers for worker nodes! Enhanced ptrace interceptor + --local-storage-capacity-isolation=false + iptables-legacy = functional worker nodes.

**ğŸŠ BREAKTHROUGH #3** (2025-11-22): Experiment 15 achieves stable k3s worker node! Post-start hook panic is NOT fatal + --flannel-backend=none = stable k3s running 15+ minutes with kubectl fully operational!

**ğŸ”¬ FUNDAMENTAL LIMITATION** (2025-11-22): Experiments 16-17 identified the exact blocker for pod execution - runc requires real kernel-backed cgroup files. Cannot be faked in userspace, FUSE blocked by gVisor, ptrace causes performance issues. **~97% of Kubernetes works, pod execution blocked by environment.**

**ğŸ” BOUNDARY CONFIRMED** (2025-11-24): Experiment 24 definitively confirmed the isolation boundary - the `runc init` subprocess runs in a completely isolated container namespace where NO workarounds can reach (LD_PRELOAD, ptrace, FUSE, or userspace files).

**Status**:
- âœ… **Control-plane**: PRODUCTION-READY (native k3s with fake CNI)
- âœ… **Worker node API layer**: 100% functional (kubectl works, 15+ min stability)
- âŒ **Pod execution**: BLOCKED (requires real kernel cgroup subsystem)

## Quick Start Commands

### âš¡ One-Command Quick Start (RECOMMENDED)

```bash
# Auto-installs tools and starts production-ready control-plane
sudo bash tools/quick-start.sh

# Or manually:
sudo bash solutions/control-plane-native/start-k3s-native.sh
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
kubectl get namespaces --insecure-skip-tls-verify
```

### Start k3s Control Plane (Production-Ready - Experiment 05) â­

```bash
# Start native k3s control-plane with fake CNI plugin
sudo bash solutions/control-plane-native/start-k3s-native.sh

# Configure kubectl
export KUBECONFIG=/tmp/k3s-control-plane/kubeconfig.yaml

# Verify it works
kubectl get namespaces

# Test with Helm
helm create testchart
helm install test ./testchart/
kubectl get all
```

### Alternative: Docker-based Control Plane (Legacy)

```bash
# Older approach using Docker (still works, but native is better)
sudo bash solutions/control-plane-docker/start-k3s-docker.sh
export KUBECONFIG=/root/.kube/config
kubectl get namespaces --insecure-skip-tls-verify
```

### Development Workflow

```bash
# Lint and validate Helm charts
helm lint ./chart/
helm template test ./chart/ --debug

# Install chart to control-plane
helm install myrelease ./chart/
kubectl get all

# Upgrade with different values
helm upgrade myrelease ./chart/ --set image.tag=v2.0

# Uninstall
helm uninstall myrelease
```

### Research Worker Node (API Layer Works, Pod Execution Blocked)

```bash
# Experiment 15: STABLE WORKER NODE â­ (15+ minute stability)
cd experiments/15-stable-wait-monitoring
bash run-wait-and-monitor.sh
# Result: k3s runs 15+ min, kubectl 100% functional, API layer works
# Limitation: Pods cannot execute (cgroup blocker)

# Experiments 16-17: Pod Execution Research (FUNDAMENTAL BLOCKER IDENTIFIED)
cd experiments/16-helm-chart-deployment
# Result: Pods reach ContainerCreating, blocked by runc cgroup requirement

cd experiments/17-inotify-cgroup-faker
# Result: Proved inotify works, but cannot fake cgroup files in userspace
# Finding: runc requires real kernel-backed cgroup files (cannot be faked)

# Experiment 13: Ultimate solution (6/6 k3s blockers resolved)
cd experiments/13-ultimate-solution
bash run-ultimate-solution.sh
# Result: All k3s startup blockers resolved, runs 20+ seconds

# Experiments 11-12: Critical discoveries
# - Exp 11: tmpfs cgroup support discovery
# - Exp 12: --local-storage-capacity-isolation=false flag

# Experiments 06-10: Alternative approaches tested
# - Exp 06: Enhanced ptrace (causes performance issues)
# - Exp 07: FUSE cgroups (gVisor blocks I/O operations)
# - Exp 08: Hybrid approach (components blocked)
# - Exp 09-10: LD_PRELOAD and bind mounts (partially successful)
```

## Repository Architecture

### Directory Structure

```
â”œâ”€â”€ research/          # Research documentation
â”‚   â”œâ”€â”€ research-question.md
â”‚   â”œâ”€â”€ methodology.md
â”‚   â”œâ”€â”€ findings.md
â”‚   â””â”€â”€ conclusions.md
â”œâ”€â”€ experiments/       # Chronological experiments (01-24)
â”‚   â”œâ”€â”€ 01-control-plane-only/
â”‚   â”œâ”€â”€ 02-worker-nodes-native/
â”‚   â”œâ”€â”€ 03-worker-nodes-docker/
â”‚   â”œâ”€â”€ 04-ptrace-interception/
â”‚   â”œâ”€â”€ 05-fake-cni-breakthrough/       # â† BREAKTHROUGH #1: Fake CNI
â”‚   â”œâ”€â”€ 06-enhanced-ptrace-statfs/
â”‚   â”œâ”€â”€ 07-fuse-cgroup-emulation/
â”‚   â”œâ”€â”€ 08-ultimate-hybrid/
â”‚   â”œâ”€â”€ 09-ld-preload-intercept/        # Library-level interception
â”‚   â”œâ”€â”€ 10-bind-mount-cgroups/          # Direct bind mount approach
â”‚   â”œâ”€â”€ 11-tmpfs-cgroup-mount/          # tmpfs discovery
â”‚   â”œâ”€â”€ 12-complete-solution/           # Flag discovery
â”‚   â”œâ”€â”€ 13-ultimate-solution/           # â† BREAKTHROUGH #2: 6/6 k3s blockers resolved
â”‚   â”œâ”€â”€ 14-*/                           # (Interim experiment)
â”‚   â”œâ”€â”€ 15-stable-wait-monitoring/      # â† BREAKTHROUGH #3: 15+ min stability
â”‚   â”œâ”€â”€ 16-helm-chart-deployment/       # Pod execution research
â”‚   â”œâ”€â”€ 17-inotify-cgroup-faker/        # â† FUNDAMENTAL BLOCKER identified
â”‚   â”œâ”€â”€ 18-23-*/                        # Additional research experiments
â”‚   â”œâ”€â”€ 24-docker-runtime-exploration/  # â† BOUNDARY CONFIRMED: runc init isolation
â”‚   â”œâ”€â”€ EXPERIMENTS-09-10-SUMMARY.md    # Creative alternatives
â”‚   â””â”€â”€ EXPERIMENTS-11-13-SUMMARY.md    # Final breakthroughs
â”œâ”€â”€ solutions/         # Production-ready implementations
â”‚   â”œâ”€â”€ control-plane-native/           # â† RECOMMENDED (Exp 05)
â”‚   â”œâ”€â”€ control-plane-docker/           # Legacy
â”‚   â””â”€â”€ worker-ptrace-experimental/     # Proof-of-concept (Exp 04)
â”œâ”€â”€ docs/              # Technical documentation
â”‚   â””â”€â”€ proposals/     # Upstream contribution proposals
â”œâ”€â”€ tools/             # Automation scripts and utilities
â”‚   â”œâ”€â”€ setup-claude.sh       # Auto-installs all tools (container runtime, k8s, helm)
â”‚   â”œâ”€â”€ quick-start.sh        # One-command cluster startup
â”‚   â””â”€â”€ README.md             # Tools documentation
â”œâ”€â”€ BREAKTHROUGH.md    # Experiment 05 discovery story
â”œâ”€â”€ PROGRESS-SUMMARY.md       # Complete research findings (Experiments 15-17)
â”œâ”€â”€ RESEARCH-CONTINUATION.md  # Experiments 06-08 summary
â”œâ”€â”€ TESTING-GUIDE.md   # Comprehensive testing procedures
â””â”€â”€ QUICK-REFERENCE.md # Fast lookup guide
```

### Key Scripts

**tools/quick-start.sh** â† ONE-COMMAND START âš¡
- Auto-starts production-ready control-plane
- Waits for cluster readiness
- Displays status and helpful examples
- Perfect for new sessions

**solutions/control-plane-native/start-k3s-native.sh** â† PRODUCTION-READY
- Native k3s control-plane with fake CNI plugin (Experiment 05)
- Starts in ~15-20 seconds
- Fully stable, runs indefinitely
- Perfect for Helm chart development

**experiments/15-stable-wait-monitoring/run-wait-and-monitor.sh** â­
- Stable k3s worker node (Experiment 15)
- 15+ minute stability, kubectl 100% functional
- API layer works completely
- Pod execution blocked by cgroup limitation

**experiments/17-inotify-cgroup-faker/cgroup-faker-inotify.sh**
- Real-time inotify-based cgroup file monitoring
- Proved inotify works in gVisor
- Identified fundamental blocker: cannot fake cgroup files in userspace
- runc requires real kernel-backed cgroup control files

**tools/setup-claude.sh** (Auto-runs via SessionStart hook)
- Installs container runtime (podman, docker CLI, buildah)
- Installs Kubernetes tools (k3s, kubectl, containerd)
- Installs development tools (helm, helm-unittest, kubectx)
- Installs research tools (inotify-tools, strace, lsof)

## Critical Technical Context

### The Fundamental Blocker (Experiments 16-17, 24)

Pod execution cannot work because:
1. âœ… k3s server starts successfully (Experiment 15)
2. âœ… kubectl operations work 100%
3. âœ… Pods get scheduled to node
4. âœ… Pods reach ContainerCreating status
5. âŒ **runc init subprocess fails** â† BLOCKED HERE
   - The `runc init` subprocess runs in an **isolated container namespace**
   - This subprocess requires `/proc/sys/kernel/cap_last_cap`
   - **Subprocess Isolation Boundary** (Experiment 24):
     - LD_PRELOAD environment variables don't propagate
     - Ptrace can only trace direct children, not sub-subprocess
     - FUSE emulation â†’ gVisor blocks I/O operations (Experiment 07)
     - Userspace files â†’ rejected as inauthentic (Experiment 17)
   - Enhanced ptrace â†’ cannot reach subprocess (Experiment 06)
6. âŒ Pod never reaches Running status

**Process Hierarchy:**
```
k3s â†’ kubelet â†’ containerd â†’ runc (parent) â†’ runc init (subprocess)
                                   â†‘              â†“
                          Workarounds work    ISOLATION BOUNDARY
                                               Workarounds STOP
```

**This cannot be worked around in userspace. Requires kernel-level support that gVisor intentionally restricts.**

### Successfully Resolved Blockers

During research, we fixed multiple issues that would have prevented k3s startup:

```bash
# /dev/kmsg workaround (from Kubernetes-in-Docker)
touch /dev/kmsg
mount --bind /dev/null /dev/kmsg

# Mount propagation fix
unshare --mount --propagation unchanged bash -c 'mount --make-rshared /'

# Image GC thresholds
--kubelet-arg="--image-gc-high-threshold=100"
--kubelet-arg="--image-gc-low-threshold=99"

# CNI plugins (copy, don't symlink)
cp /usr/lib/cni/host-local /opt/cni/bin/host-local
```

These fixes brought us to the final blocker (cAdvisor filesystem incompatibility).

### Worker Node Research Approaches

**Experiment 04: Basic Ptrace** (Proven Concept)
- Intercepts open() and openat() syscalls
- Redirects /proc/sys/* paths to /tmp/fake-procsys/*
- Enables kubelet to start, but unstable (30-60s runtime)
- Location: solutions/worker-ptrace-experimental/

**Experiment 06: Enhanced Ptrace** (Testing)
- Extends Exp 04 with statfs() syscall interception
- Spoofs 9p filesystem type as ext4 to fool cAdvisor
- Expected: Extended stability beyond 60 seconds
- Location: experiments/06-enhanced-ptrace-statfs/

**Experiment 07: FUSE cgroup Emulation** (Testing)
- Virtual cgroupfs filesystem using FUSE
- Provides cgroup files cAdvisor needs for metrics
- Clean alternative to ptrace for cgroup access
- Location: experiments/07-fuse-cgroup-emulation/

**Experiments 09-10: Creative Alternatives** (Research)
- Exp 09: LD_PRELOAD library interception (works perfectly, but k3s is statically-linked Go)
- Exp 10: Direct bind mounts (proven viable in gVisor!)
- Identified exact upstream cAdvisor fix needed (1-line change)
- Location: experiments/09-ld-preload-intercept/, experiments/10-bind-mount-cgroups/
- Summary: experiments/EXPERIMENTS-09-10-SUMMARY.md

**Experiments 11-12: Flag Discoveries** (Breakthroughs)
- Exp 11: tmpfs is supported by cAdvisor (was mounting 9p files incorrectly)
- Exp 12: --local-storage-capacity-isolation=false ELIMINATES cAdvisor error!
- Location: experiments/11-tmpfs-cgroup-mount/, experiments/12-complete-solution/
- Summary: experiments/EXPERIMENTS-11-13-SUMMARY.md

**Experiment 13: Ultimate Solution** â­ (MAJOR SUCCESS)
- Combines ALL working techniques from all experiments
- Enhanced ptrace interceptor with dynamic /proc/sys/* redirection
- --local-storage-capacity-isolation=false (Exp 12)
- iptables-legacy workaround
- Fake CNI plugin (Exp 05)
- Location: experiments/13-ultimate-solution/
- **Result: 6/6 fundamental blockers RESOLVED!**
- kube-proxy starts successfully (first time ever!)
- API server handles requests
- k3s runs for ~20 seconds
- Remaining: API server timing issue (solvable optimization challenge)

## What Works vs What Doesn't

### âœ… Control-Plane-Only Mode (Fully Functional)

- API Server, Scheduler, Controller Manager
- CoreDNS
- kubectl operations (create, get, describe, delete)
- Helm install/upgrade/uninstall
- Resource creation (Deployments, Services, ConfigMaps, CRDs)
- RBAC configuration
- Server-side dry runs
- API compatibility validation

### âŒ Not Functional (Requires External Cluster)

- Pod execution (pods stay Pending - no worker node)
- Container logs (kubectl logs)
- Container exec (kubectl exec)
- Service networking (no endpoints without running pods)
- Ingress routing
- Persistent volume mounting

## Common Development Tasks

### Helm Chart Development

Control-plane-only mode provides everything needed:

```bash
# Lint chart
helm lint ./chart/

# Validate template rendering
helm template test ./chart/ --debug

# Test installation
helm install myrelease ./chart/

# Verify resources created
kubectl get all

# Test RBAC
kubectl auth can-i list pods --as=system:serviceaccount:default:myapp

# Server-side validation
kubectl apply -f deployment.yaml --dry-run=server
```

### Testing Changes

```bash
# Check k3s status
systemctl status k3s

# View k3s logs
journalctl -u k3s -f

# Restart k3s
systemctl restart k3s

# Stop k3s
systemctl stop k3s

# Re-run setup (quick-start)
sudo bash tools/quick-start.sh
```

### Accessing the Cluster

```bash
# Set kubeconfig (auto-configured by SessionStart hook)
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

# Skip TLS verification (self-signed cert)
kubectl get nodes --insecure-skip-tls-verify

# Or configure in kubeconfig
kubectl config set-cluster default --insecure-skip-tls-verify=true
```

## Environment Details

**Target Platform**: Claude Code web sessions
- **Sandbox**: gVisor (runsc) with restricted kernel access
- **Filesystem**: 9p (Plan 9 Protocol) virtual filesystem
- **OS**: Linux 4.4.0
- **Capabilities**: Limited CAP_SYS_ADMIN, CAP_SYS_PTRACE available
- **k3s Version**: v1.33.5-k3s1 (later v1.34.1)

**Filesystem Detection**:
```bash
mount | grep " / "
# Output: runsc-root on / type 9p (rw,relatime,trans=fd,rfd=3,wfd=3)
```

## Research Documentation

Each experiment follows structured documentation:
1. **Hypothesis** - Expected outcome
2. **Method** - Exact commands executed
3. **Results** - Actual outcome
4. **Analysis** - Root cause analysis
5. **Next Steps** - Follow-up actions

**Primary Documents**:
- research/research-question.md - Original research question
- research/methodology.md - Experimental approach
- research/findings.md - Detailed results with evidence
- research/conclusions.md - Final recommendations
- docs/technical-deep-dive.md - Complete technical summary

## Related Upstream Issues

This limitation is known in the community:
- [k3s-io/k3s#8404](https://github.com/k3s-io/k3s/issues/8404) - Same cAdvisor error
- [kubernetes-sigs/kind#3839](https://github.com/kubernetes-sigs/kind/issues/3839) - Filesystem compatibility
- Multiple GitPod/Cloud IDE reports with same issue

## Recommendations for Future Work

### For Helm Chart Development
Use control-plane-only solution (solutions/control-plane-docker) - fully functional and stable.

### For Full Integration Testing
- External k3s cluster
- k3d/kind on local machine (requires Docker Desktop VM)
- Cloud Kubernetes cluster (EKS, GKE, AKS)

### For Research Continuation
- Investigate cAdvisor patches to support 9p
- Explore eBPF-based filesystem virtualization
- Test with newer gVisor releases
- Investigate alternative container runtimes

## SessionStart Hook

The `.claude/hooks/SessionStart` script automatically runs when a Claude Code session starts:
- Runs tools/setup-claude.sh (if CLAUDE_CODE_REMOTE=true)
- Installs container runtime (podman, docker CLI, buildah)
- Installs Kubernetes tools (k3s, kubectl, containerd)
- Installs development tools (helm, helm-unittest, kubectx, kubens)
- Installs research tools (inotify-tools, strace, lsof)
- Configures KUBECONFIG environment variable
- Displays production-ready solution instructions
- Shows quick-start commands and documentation links

## Important Caveats

**When modifying scripts**:
- Test in Claude Code web environment (CLAUDE_CODE_REMOTE=true)
- Ensure idempotency (safe to run multiple times)
- This environment is sandboxed - some operations will fail by design

**Security context**:
- Research conducted in authorized sandbox
- No attempt to break out of sandbox
- Focus on legitimate development workflows
- Ethical use of ptrace for syscall interception

**Performance notes**:
- Control-plane startup: ~30 seconds
- Memory usage: ~200-300MB
- Ptrace overhead: 2-5x slowdown on intercepted syscalls
